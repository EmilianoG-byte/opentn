{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import rqcopt as oc\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import jax\n",
    "from jax import config\n",
    "config.update(\"jax_enable_x64\", True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the setup for the problem\n",
    "from opentn.transformations import create_kitaev_liouvillians, exp_operator_dt, factorize_psd, super2choi\n",
    "d, N, gamma = 2, 4, 1\n",
    "tau = 4\n",
    "dim = d**N\n",
    "Lvec, Lvec_odd, Lvec_even, Lnn = create_kitaev_liouvillians(N=N, d=d, gamma=gamma)\n",
    "superops_exp = []\n",
    "for i, op in enumerate([Lvec, Lvec_odd, Lvec_even]):\n",
    "    if i == 1:\n",
    "        superops_exp.append(exp_operator_dt(op, tau/2, 'jax'))\n",
    "    else:\n",
    "        superops_exp.append(exp_operator_dt(op, tau, 'jax'))\n",
    "exp_Lvec, exp_Lvec_odd, exp_Lvec_even = superops_exp\n",
    "\n",
    "tol = 1e-12\n",
    "\n",
    "X1 = factorize_psd(psd=super2choi(exp_Lvec_odd), tol=tol)\n",
    "X2 = factorize_psd(psd=super2choi(exp_Lvec_even), tol=tol)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n",
      "0.0\n",
      "0.0\n"
     ]
    }
   ],
   "source": [
    "# NOTE: Everything is real so the norm of the imaginary part is completely zero for the gradient and the Xi themselves\n",
    "for op in [exp_Lvec, exp_Lvec_odd, exp_Lvec_even]:\n",
    "    print(np.linalg.norm(op.imag))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "# we have to start working with X that are (out, k, in) instead of (out, in, k)\n",
    "# first step: get the k for each layer.\n",
    "k1 = np.linalg.matrix_rank(X1)\n",
    "k2 = np.linalg.matrix_rank(X2)\n",
    "\n",
    "def split_matrix_svd(A, chi_max=2, eps=1e-9):\n",
    "    \"\"\"\n",
    "    Split a matrix by singular value decomposition,\n",
    "    and truncate small singular values based on tolerance.\n",
    "    \"\"\"\n",
    "    assert A.ndim == 2\n",
    "    u, s, v = np.linalg.svd(A, full_matrices=False)\n",
    "    # truncate small singular values\n",
    "    chi_keep = min(chi_max, np.sum(s > eps))\n",
    "    assert chi_keep >=1\n",
    "\n",
    "    idx_keep = np.argsort(s)[::-1][:chi_keep]  # keep the largest `chivC` singular values\n",
    "\n",
    "    u = u[:, idx_keep]\n",
    "    v = v[idx_keep, :]\n",
    "    s = s[idx_keep]\n",
    "    return u, s, v\n",
    "\n",
    "\n",
    "def factorize_psd_truncated(psd, chi_max=2, eps=1e-9):\n",
    "    \"factorize psd matrix truncating the singular values based on parameters\"\n",
    "    x, s, xdg = split_matrix_svd(psd, chi_max, eps)\n",
    "    return x@np.diag(np.sqrt(s))\n",
    "\n",
    "\n",
    "xs_truncated = []\n",
    "for X, op in zip([X1,X2], [exp_Lvec_odd, exp_Lvec_even]):\n",
    "    k = np.linalg.matrix_rank(X)\n",
    "    x_truncated = factorize_psd_truncated(psd=super2choi(op), chi_max=k)\n",
    "    C_trnc = x_truncated@x_truncated.conj().T\n",
    "    print(np.allclose(C_trnc, X@X.conj().T))\n",
    "    xs_truncated.append(x_truncated)\n",
    "x1_truncated, x2_truncated = xs_truncated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(256, 4)\n",
      "(64, 16)\n",
      "True\n",
      "(256, 2)\n",
      "(32, 16)\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "# now we need to transform the matrices X we constructed to match the orthogonality condition \n",
    "def choi2ortho(x:np.ndarray):\n",
    "    \"transform the x matrices that factorize a choi matrix into its orthogonal form\"\n",
    "    # reshape the x matrix from (out, in, k) to (out, k, in)\n",
    "    dim = int(np.sqrt(x.shape[0]))\n",
    "    k = x.shape[1]\n",
    "    x = np.reshape(x, [dim, dim, k])\n",
    "    x = x.swapaxes(1,2).reshape([dim*k, dim])\n",
    "    return x\n",
    "\n",
    "for x_truncated in [x1_truncated, x2_truncated]:\n",
    "    print(x_truncated.shape)\n",
    "    x_ortho = choi2ortho(x_truncated)\n",
    "    print(x_ortho.shape)\n",
    "    print(np.allclose(x_ortho.conj().T@x_ortho, np.eye(dim)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.09591767023235143\n",
      "0.09591767023235143\n",
      "(2560,)\n"
     ]
    }
   ],
   "source": [
    "# let's create the gradient and see if we can make it into a matrix\n",
    "\n",
    "# step 1: create the cost function.\n",
    "# option 1: as we have done it so far, with superoperators and composing them with @. model ys\n",
    "# option 2: with choi matrices and the choi composition function. model cs\n",
    "xi_init = [x1_truncated, x2_truncated, x1_truncated]\n",
    "from opentn.optimization import frobenius_norm, model_Ys, compute_loss\n",
    "print(compute_loss(xi=xi_init, loss_fn=frobenius_norm, model=model_Ys, exact=exp_Lvec))\n",
    "# but f should only accept the list of xi as input, meaning that the exact should be given already.\n",
    "f = lambda xi: frobenius_norm(model_Ys(xi), exp_Lvec)\n",
    "print(f(xi_init))\n",
    "# we leave the retraction for later. For now let's focus on the gradient and hessian.\n",
    "# get the euclidean gradient using jax\n",
    "\n",
    "from opentn.transformations import vectorize\n",
    "\n",
    "# the riemannian gradient is obtained projecting to tangent space. \n",
    "def project(X, Z):\n",
    "    \"project Z vector onto tangent space at X on manifold\"\n",
    "    return Z - 0.5 * X @ (X.conj().T @ Z + Z.conj().T @ X)\n",
    "\n",
    "def rgrad_f(xi):\n",
    "    \"compute riemannian gradient for all xi, returning a list\"\n",
    "    Zi = jax.grad(f)(xi)\n",
    "    return [project(X, Z)\n",
    "    for X,Z in zip(xi, Zi)]\n",
    "\n",
    "def rgrad_f_vec(xi):\n",
    "    \"compute the vectorized gradient for all xi\"\n",
    "    return np.vstack([\n",
    "            vectorize(grad) \n",
    "    for grad in rgrad_f(xi)]).reshape(-1)\n",
    "\n",
    "print(rgrad_f_vec(xi_init).shape)\n",
    "\n",
    "# now it is the hessian which remains a question. Let us just create the directed matrix as I think it should be, and then we see if it works\n",
    "def metric(delta1, delta2, X):\n",
    "    \"\"\"\n",
    "    riemannian metric between delta1 and delta2 in tangent space at X in manifold. \n",
    "    From https://arxiv.org/abs/2112.05176 eq. 24\n",
    "    \"\"\"\n",
    "    dim = X.shape[0]\n",
    "    gamma = np.eye(dim) - 0.5 * (X@X.conj().T)\n",
    "    return np.trace(delta1.conj().T@gamma@delta2).real\n",
    "\n",
    "def hvp(xi, v):\n",
    "    \"from https://jax.readthedocs.io/en/latest/notebooks/autodiff_cookbook.html#hessian-vector-products-with-grad-of-grad\"\n",
    "    return project(\n",
    "        jax.grad(lambda xi: metric(rgrad_f(xi), v, x))(xi[0]),\n",
    "        x[0])\n",
    "\n",
    "\n",
    "\n",
    "# v = np.zeros_like(xi_init[0].reshape(-1))\n",
    "# v[0] = 1\n",
    "# v = v.reshape(xi_init[0].shape)\n",
    "# hvp(xi_init, v=v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2560,)"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def hessian_riem(xi):\n",
    "    primals, f_vjp = jax.jvp(fun=rgrad_f, primals=[xi], tangents=[xi])\n",
    "    # for p in primals:\n",
    "    #     print(p.shape)\n",
    "    hess_xi = [project(X, Z) for X,Z in zip(xi, primals)]\n",
    "    return hess_xi\n",
    "\n",
    "def hessian_riem_vec(xi):\n",
    "    return np.vstack([\n",
    "            vectorize(hess) \n",
    "    for hess in hessian_riem(xi)]).reshape(-1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(256, 4)\n",
      "(256, 2)\n",
      "(256, 4)\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-7.311165756268884"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lambda metric(rgrad_f(xi_init)[0], v, xi_init[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.7305042292865611"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grads = jax.grad(f)(xi_init)\n",
    "metric(grads[0], grads[2], xi_init[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 2, 3, 4, 0, 1, 2, 0, 1])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# lets concatenate vectors into one long vector\n",
    "a = np.arange(5).reshape((5,1))\n",
    "b =  np.arange(3).reshape((3,1))\n",
    "c = np.arange(2).reshape((2,1))\n",
    "np.vstack([a,b,c]).reshape(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "oc.riemannian_trust_region_optimize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "oc.retract_unitary_list()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "thesis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
